{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c372a801-6dd8-4659-8431-d893c608d1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 1 ------------------------------\n",
    "# Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. \n",
    "# Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fb5e3c2-f6a1-44c8-abac-52989151f629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d225c69-21fd-4129-88f8-7b601ee6d7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f4be80a-b6eb-4b47-9853-cee463e03b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_chars = sorted(list(set(\"\".join(words)))) + ['.']\n",
    "used_chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f15a87a0-e41f-4858-aa95-11ff28976773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctoi = {e:i for i, e in enumerate(used_chars)}\n",
    "itoc = {i:e for i, e in enumerate(used_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2f5c148-6eba-46d5-9411-b711d2664416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 26, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    used_chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(used_chars, used_chars[1:], used_chars[2:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        ix3 = ctoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24e2e2d1-92ff-4c33-bdcf-7aecb136cdfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b44f897-c5a5-4b7d-a3a0-6813e78da1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mi\n",
      "blareher\n",
      "phia\n",
      "faber\n",
      "ey\n",
      "rialilastem\n",
      "taylipolue\n",
      "quim\n",
      "ten\n",
      "qaklyned\n",
      "iniwhcvdnhtonnstyleis\n",
      "urna\n",
      "ba\n",
      "il\n",
      "odomihad\n",
      "zolcngtqj\n",
      "tt\n",
      "pararyolah\n",
      "manshvior\n",
      "ile\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(123456789)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = [ctoi['.'], torch.randint(0, 26, (1,), generator=g).item()]\n",
    "\n",
    "    while True:\n",
    "        p = P[out[-2], out[-1]]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == 26:\n",
    "            break\n",
    "        out.append(ix)\n",
    "    \n",
    "    print(\"\".join(itoc[i] for i in out)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddc4f865-ca92-4e4c-a97d-3ad45b0fa2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood= -410414.96875\n",
      "nll= 410414.96875\n",
      "average negative likelihood: 2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "# for w in [\"david\"]:\n",
    "    used_chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(used_chars, used_chars[1:], used_chars[2:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        ix3 = ctoi[ch3]\n",
    "        prob = P[ix1, ix2, ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f\"{ch1}{ch2}{ch3}: {prob:.4f} {logprob:.4f}\")\n",
    "        \n",
    "print(f\"log_likelihood= {log_likelihood.item()}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"nll= {nll.item()}\")\n",
    "print(f'average negative likelihood: {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6f64b86-6f45-4baa-b526-97ab8ee846bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 2 ------------------------------\n",
    "# Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "# Train the bigram and trigram models only on the training set. \n",
    "# Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5032dfb2-7afd-4a98-a127-36cef6a394c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_train 25627\n",
      "words_dev 3203\n",
      "words_test 3203\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words_len = len(words)\n",
    "words_train, words_dev, words_test = [], [], []\n",
    "\n",
    "for _ in range(int(words_len*0.1)):\n",
    "    rand_word_i = torch.randint(0, len(words), (1,)).item()\n",
    "    words_dev.append(words[rand_word_i])\n",
    "    del words[rand_word_i]\n",
    "    \n",
    "for _ in range(int(words_len*0.1)):\n",
    "    rand_word_i = torch.randint(0, len(words), (1,)).item()\n",
    "    words_test.append(words[rand_word_i])\n",
    "    del words[rand_word_i]\n",
    "    \n",
    "words_train = words.copy()\n",
    "\n",
    "print(\"words_train\", len(words_train))\n",
    "print(\"words_dev\", len(words_dev))\n",
    "print(\"words_test\", len(words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a730743c-6293-4d46-93ee-edbf0bc796c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL of bigram model on words_dev: 2.463676929473877\n",
      "Average NLL of bigram model on words_test: 2.4499571323394775\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "\n",
    "chars = sorted(list(set(''.join(words_train))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "N = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "for w in words_train:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1\n",
    "        \n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdim=True)\n",
    "\n",
    "for dataset, dataset_name in [(words_dev,\"words_dev\"), (words_test, \"words_test\")]:\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "    for w in dataset:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            prob = P[ix1, ix2]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "\n",
    "    nll = -log_likelihood # negative log likelihood\n",
    "    print(f'Average NLL of bigram model on {dataset_name}: {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eae60376-47dd-4d31-81ad-0da6c1a710ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL of trigram model on words_dev: 2.1346964836120605\n",
      "Average NLL of trigram model on words_test: 2.1141598224639893\n"
     ]
    }
   ],
   "source": [
    "# Trigram\n",
    "\n",
    "used_chars = sorted(list(set(\"\".join(words)))) + ['.']\n",
    "used_chars[:10]\n",
    "ctoi = {e:i for i, e in enumerate(used_chars)}\n",
    "itoc = {i:e for i, e in enumerate(used_chars)}\n",
    "\n",
    "N = torch.zeros((27, 26, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    used_chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(used_chars, used_chars[1:], used_chars[2:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        ix3 = ctoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "        \n",
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdim=True)\n",
    "\n",
    "for dataset, dataset_name in [(words_dev,\"words_dev\"), (words_test, \"words_test\")]:\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for w in dataset:\n",
    "        used_chars = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(used_chars, used_chars[1:], used_chars[2:]):\n",
    "            ix1 = ctoi[ch1]\n",
    "            ix2 = ctoi[ch2]\n",
    "            ix3 = ctoi[ch3]\n",
    "            prob = P[ix1, ix2, ix3]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "\n",
    "    nll = -log_likelihood\n",
    "    print(f'Average NLL of trigram model on {dataset_name}: {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5b99337-4d6c-4d87-88fa-445b1d1f677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results: Trigram is significantly better on both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "befbf2e9-68e6-4ef7-8712-4d8a4297dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 3 ------------------------------\n",
    "# Use the dev set to tune the strength of smoothing (or regularization) \n",
    "# for the trigram model - i.e. try many possibilities and see which one \n",
    "# works best based on the dev set loss. \n",
    "\n",
    "# What patterns can you see in the train and dev set loss as you tune this strength? \n",
    "\n",
    "# Take the best setting of the smoothing and evaluate on the test set \n",
    "# once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "082ccaf9-766f-4e3d-9306-242ba2c4764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram\n",
    "\n",
    "used_chars = sorted(list(set(\"\".join(words)))) + ['.']\n",
    "used_chars[:10]\n",
    "ctoi = {e:i for i, e in enumerate(used_chars)}\n",
    "itoc = {i:e for i, e in enumerate(used_chars)}\n",
    "\n",
    "N = torch.zeros((27, 26, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    used_chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(used_chars, used_chars[1:], used_chars[2:]):\n",
    "        ix1 = ctoi[ch1]\n",
    "        ix2 = ctoi[ch2]\n",
    "        ix3 = ctoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25b77daf-1c4f-422c-adf3-963be94e3d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANLL with 1 smoothing on words_dev: 2.1346964836120605 on words_test: 2.1141598224639893\n",
      "ANLL with 2 smoothing on words_dev: 2.1570229530334473 on words_test: 2.1363918781280518\n",
      "ANLL with 3 smoothing on words_dev: 2.1773312091827393 on words_test: 2.1566004753112793\n",
      "ANLL with 4 smoothing on words_dev: 2.1958227157592773 on words_test: 2.1750497817993164\n",
      "ANLL with 5 smoothing on words_dev: 2.212879180908203 on words_test: 2.192089319229126\n",
      "ANLL with 10 smoothing on words_dev: 2.2833731174468994 on words_test: 2.262901782989502\n",
      "ANLL with 20 smoothing on words_dev: 2.3840253353118896 on words_test: 2.364640712738037\n",
      "ANLL with 50 smoothing on words_dev: 2.563149929046631 on words_test: 2.5466408729553223\n",
      "ANLL with 100 smoothing on words_dev: 2.721606969833374 on words_test: 2.708057165145874\n",
      "ANLL with 200 smoothing on words_dev: 2.879863739013672 on words_test: 2.8695201873779297\n",
      "ANLL with 500 smoothing on words_dev: 3.0565648078918457 on words_test: 3.0501229763031006\n",
      "ANLL with 1000 smoothing on words_dev: 3.1519503593444824 on words_test: 3.1478445529937744\n",
      "ANLL with 10000 smoothing on words_dev: 3.2775511741638184 on words_test: 3.2769668102264404\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for smoothing in [1, 2, 3, 4, 5, 10, 20, 50, 100, 200, 500, 1000, 10000]:\n",
    "\n",
    "    P = (N+smoothing).float()\n",
    "    P /= P.sum(2, keepdim=True)\n",
    "\n",
    "    for dataset, dataset_name in [(words_dev,\"words_dev\"), \n",
    "                                  (words_test, \"words_test\")]:\n",
    "        log_likelihood = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for w in dataset:\n",
    "            used_chars = ['.'] + list(w) + ['.']\n",
    "            for ch1, ch2, ch3 in zip(used_chars, used_chars[1:], used_chars[2:]):\n",
    "                ix1 = ctoi[ch1]\n",
    "                ix2 = ctoi[ch2]\n",
    "                ix3 = ctoi[ch3]\n",
    "                prob = P[ix1, ix2, ix3]\n",
    "                logprob = torch.log(prob)\n",
    "                log_likelihood += logprob\n",
    "                n += 1\n",
    "\n",
    "        nll = -log_likelihood\n",
    "        results[str(smoothing) + dataset_name] = nll/n\n",
    "        \n",
    "    print(f'ANLL with {smoothing} smoothing on words_dev: {results[str(smoothing) + \"words_dev\"]} on words_test: {results[str(smoothing) + \"words_test\"]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "473227fe-0f6b-4c86-8614-e4fc491c9662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patterns: The words_dev has nearly identical results to words_test\n",
    "# Best loss: 2.1142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01947f88-502c-43eb-9b96-c7c5171b3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 4 ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5151ea12-c60f-4460-a798-744dc06518a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. \n",
    "# Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2247d0ad-9dd9-4132-b3c1-289d6a771957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "34654b9b-5708-485a-ba3b-b498fdb03cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 228146\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f'number of elements: {num}')\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c576ee12-e17d-4240-8177-669a9811ccb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.768618583679199\n",
      "2.701957941055298\n",
      "2.5923783779144287\n",
      "2.553252696990967\n",
      "2.534149408340454\n",
      "2.5233426094055176\n",
      "2.516758680343628\n",
      "2.512606620788574\n",
      "2.510012149810791\n",
      "2.508526563644409\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    # xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    # logits = xenc @ W # predict log-counts\n",
    "    logits = W[xs]\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean() # The '+ 0.01 * (W**2).mean()' part is for regularization to push the elements of W to 0\n",
    "                                                                             # In other words it's reguralization on neural network\n",
    "    if k%10==0:    \n",
    "        print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data -= (51-k/2.0) * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8000621f-4a6a-4a3c-9857-1eaeee8d0e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Yes, one_hot can be replaced with W[xs] in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b00f7a5b-b557-4ac1-856c-c471b1e232fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor\n",
      "axx\n",
      "minaynnnyles\n",
      "kondmaisah\n",
      "anchthizarie\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5): \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5229cbc0-3d4c-48cf-9aa2-e07926db6167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 5 ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3a30063e-00a0-42d9-9327-27817509ddd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look up and use F.cross_entropy instead. \n",
    "# You should achieve the same result. \n",
    "# Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1b404e53-7265-4291-88c9-0c790b2d50c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "513a7249-7cb6-4a12-b3cd-9eb15da63b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 228146\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f'number of elements: {num}')\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fb9b9753-fc9b-4ad6-aded-86da445edab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953332901001\n",
      "2.6944985389709473\n",
      "2.5831985473632812\n",
      "2.542685031890869\n",
      "2.5225307941436768\n",
      "2.510931968688965\n",
      "2.503763437271118\n",
      "2.4991891384124756\n",
      "2.496304750442505\n",
      "2.494642496109009\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    logits = W[xs]\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    \n",
    "    if k%10==0:    \n",
    "        print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data -= (51-k/2.0) * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "630c4a99-f8ba-4b2d-ba1f-7648a98c59d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor\n",
      "axx\n",
      "minaymoryles\n",
      "kondmaisah\n",
      "anchthizarie\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b5534-2435-4934-887a-1aa2966ca261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
