{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dec0cc7-badd-4987-beaf-c30f8157b8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 1 ------------------------------\n",
    "# Tune the hyperparameters of the training to beat my best validation loss of 2.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4a1492e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ef7b6543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a3cdf4ac-e1c1-49a6-a78f-c1f7f2b91868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappins to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "772e7237-9c3a-41f0-85ba-af355c4b734e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "    \n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "94c9f124-7846-4b5b-8338-32bd716291f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_size = 20\n",
    "hidden_layer_size = 100\n",
    "mini_batch_size = 1024\n",
    "regularization = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d700b3a7-fdc3-4fb4-addb-a85374908f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, emb_size), generator=g)\n",
    "W1 = torch.randn((3*emb_size, hidden_layer_size), generator=g)\n",
    "b1 = torch.randn(hidden_layer_size, generator=g)\n",
    "W2 = torch.randn((hidden_layer_size,27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c03f8f63-ee85-4f15-add4-01637ce2d9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9367"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d294b0e6-26a1-4472-b569-2db6d47e036b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c95b92a4-b0e6-470b-9e47-826b870d6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(setX, setY):\n",
    "    emb = C[setX]\n",
    "    h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, setY) + regularization * (W1**2).mean() + regularization * (W2**2).mean()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6206a942-5815-4226-9228-06fd1708e664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It count: 0, train loss: 22.164934158325195, dev loss: 22.110742568969727\n",
      "It count: 5000, train loss: 3.5490193367004395, dev loss: 3.562366008758545\n",
      "It count: 10000, train loss: 2.8175747394561768, dev loss: 2.8264577388763428\n",
      "It count: 15000, train loss: 2.515599250793457, dev loss: 2.5256471633911133\n",
      "It count: 20000, train loss: 2.3677351474761963, dev loss: 2.3796639442443848\n",
      "It count: 25000, train loss: 2.2924695014953613, dev loss: 2.3065078258514404\n",
      "It count: 30000, train loss: 2.2537903785705566, dev loss: 2.2689571380615234\n",
      "It count: 35000, train loss: 2.2321958541870117, dev loss: 2.2479729652404785\n",
      "It count: 40000, train loss: 2.2178659439086914, dev loss: 2.2361550331115723\n",
      "It count: 45000, train loss: 2.208737373352051, dev loss: 2.2280123233795166\n",
      "It count: 50000, train loss: 2.200308084487915, dev loss: 2.2216310501098633\n",
      "It count: 55000, train loss: 2.194851875305176, dev loss: 2.216409921646118\n",
      "It count: 60000, train loss: 2.189108371734619, dev loss: 2.212928533554077\n",
      "It count: 65000, train loss: 2.184680223464966, dev loss: 2.2094974517822266\n",
      "It count: 70000, train loss: 2.181328296661377, dev loss: 2.2070207595825195\n",
      "It count: 75000, train loss: 2.178009033203125, dev loss: 2.2032241821289062\n",
      "It count: 80000, train loss: 2.1749446392059326, dev loss: 2.2022087574005127\n",
      "It count: 85000, train loss: 2.17189884185791, dev loss: 2.199817419052124\n",
      "It count: 90000, train loss: 2.1699230670928955, dev loss: 2.1985769271850586\n",
      "It count: 95000, train loss: 2.1676645278930664, dev loss: 2.1965973377227783\n",
      "It count: 100000, train loss: 2.1653435230255127, dev loss: 2.1945722103118896\n",
      "It count: 105000, train loss: 2.1616621017456055, dev loss: 2.1908178329467773\n",
      "It count: 110000, train loss: 2.1614136695861816, dev loss: 2.190443515777588\n",
      "It count: 115000, train loss: 2.1611433029174805, dev loss: 2.1903088092803955\n",
      "It count: 120000, train loss: 2.160943031311035, dev loss: 2.190300464630127\n",
      "It count: 125000, train loss: 2.160679578781128, dev loss: 2.18989896774292\n",
      "It count: 130000, train loss: 2.1604347229003906, dev loss: 2.190194845199585\n",
      "It count: 135000, train loss: 2.16023850440979, dev loss: 2.189913749694824\n",
      "It count: 140000, train loss: 2.160128116607666, dev loss: 2.1896767616271973\n",
      "It count: 145000, train loss: 2.1598713397979736, dev loss: 2.1898388862609863\n",
      "It count: 150000, train loss: 2.1596310138702393, dev loss: 2.1893086433410645\n",
      "It count: 155000, train loss: 2.159531831741333, dev loss: 2.1892380714416504\n",
      "It count: 160000, train loss: 2.1592071056365967, dev loss: 2.1892919540405273\n",
      "It count: 165000, train loss: 2.159045934677124, dev loss: 2.1890504360198975\n",
      "It count: 170000, train loss: 2.1588754653930664, dev loss: 2.1888160705566406\n",
      "It count: 175000, train loss: 2.1587507724761963, dev loss: 2.188671827316284\n",
      "It count: 180000, train loss: 2.1585822105407715, dev loss: 2.1885287761688232\n",
      "It count: 185000, train loss: 2.158374309539795, dev loss: 2.188544273376465\n",
      "It count: 190000, train loss: 2.158170223236084, dev loss: 2.1880452632904053\n",
      "It count: 195000, train loss: 2.158083438873291, dev loss: 2.1884236335754395\n"
     ]
    }
   ],
   "source": [
    "loss_tr = []\n",
    "loss_dev = []\n",
    "stepi = []\n",
    "\n",
    "it_count=200000\n",
    "report_every_it = 1000\n",
    "print_every_it = 5000\n",
    "\n",
    "for i in range(it_count):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (mini_batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) + regularization * (W1**2).mean() + regularization * (W2**2).mean()\n",
    "    # print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1 if i < 100000 else 0.01 \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i%(report_every_it) == 0:\n",
    "        stepi.append(i)\n",
    "        loss_tr.append(total_loss(Xtr, Ytr))\n",
    "        loss_dev.append(total_loss(Xdev, Ydev))\n",
    "        if i%(print_every_it) == 0:\n",
    "            print(f\"It count: {i}, train loss: {loss_tr[-1]}, dev loss: {loss_dev[-1]}\")\n",
    "        \n",
    "        \n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2a36a3e3-7dd6-4667-9e5f-14285cc4c7a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x177b23280>]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, loss_tr)\n",
    "plt.plot(stepi, loss_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e041f33f-0ecb-4b53-9492-0dc8024cc0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0875, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d774e34b-1ce7-49f4-9ea3-e6df286f4e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1185, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ae5fd140-1c7d-4d33-9d74-1c8b10905647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training took 04:18 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3d56dd4a-5b75-4102-b080-e1f5affe96f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "samples = []\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        logits = None\n",
    "        \n",
    "        h = torch.tanh(emb.view(1, -1) @ a_W1 + a_b1)\n",
    "        logits = h @ a_W2 + a_b2\n",
    "            \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "        if ix==0:\n",
    "            break\n",
    "        out.append(ix)\n",
    "\n",
    "    samples.append(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "fcd46223-24f1-4109-a642-0184076b312e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample len: 100\n",
      "New Words len: 83\n",
      "New words: ['deabe', 'ahalie', 'aileik', 'aha', 'eilie', 'aia', 'eeleahk', 'aha', 'icheeleiha', 'iah', 'iei', 'kha', 'keiged', 'aea', 'eadee', 'aakaigeagif', 'bai', 'haliel', 'alaheah', 'dhad', 'ahaila', 'kaedeil', 'fie', 'aheaika', 'bhaab', 'aalh', 'khaliellee', 'ablieahhakeechee', 'aia', 'aed', 'ehleileahaleegedlieiahea', 'ebhaleia', 'aahali', 'haae', 'ael', 'keilia', 'fiailcalha', 'fabe', 'eela', 'eehaliah', 'aiah', 'eahi', 'akha', 'geich', 'aihleah', 'eabl', 'aea', 'aaddelia', 'addelkaha', 'cahlacdealiahali', 'hagh', 'had', 'ahahili', 'dha', 'kahaye', 'eck', 'ahal', 'fic', 'aed', 'deelie', 'iei', 'had', 'dahae', 'alei', 'fali', 'eig', 'alkilleaheilea', 'aad', 'eie', 'iab', 'aalief', 'eiae', 'aea', 'geadeaha', 'aia', 'aeh', 'chaeik', 'aha', 'abhaiciawa', 'jaidelie', 'aediegha', 'lha', 'deah']\n"
     ]
    }
   ],
   "source": [
    "new_words = [sample for sample in samples if sample not in words]\n",
    "\n",
    "print(f\"Sample len: {len(samples)}\")\n",
    "print(f\"New Words len: {len(new_words)}\")\n",
    "\n",
    "print(\"New words:\", new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbc84d54-6ead-478f-9291-de6f2493e4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 2 ------------------------------\n",
    "# I was not careful with the intialization of the network in this video. \n",
    "# (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? \n",
    "# (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "\n",
    "# (1) The starting loss is 7, much lower than with non-uniform initial parameters, however the loss after 200k iterations is 2.4677 (much worse)\n",
    "# (2) I assume it can be done by initializing C, W and b tensors with zeros and ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "544c7eb8-ef20-4f49-82be-45cbe608f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.ones((27, emb_size))\n",
    "W1 = torch.ones((3*emb_size, hidden_layer_size))\n",
    "b1 = torch.zeros(hidden_layer_size)\n",
    "W2 = torch.ones((hidden_layer_size,27))\n",
    "b2 = torch.zeros(27)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9d7a346-cc59-4e29-a5a1-17c1aaa2c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It count: 0, train loss: 6.980022430419922, dev loss: 6.979219436645508\n",
      "It count: 5000, train loss: 4.307507038116455, dev loss: 4.306113243103027\n",
      "It count: 10000, train loss: 3.460493803024292, dev loss: 3.4585511684417725\n",
      "It count: 15000, train loss: 3.1187005043029785, dev loss: 3.116990089416504\n",
      "It count: 20000, train loss: 2.9700562953948975, dev loss: 2.968836545944214\n",
      "It count: 25000, train loss: 2.915227174758911, dev loss: 2.9129257202148438\n",
      "It count: 30000, train loss: 2.8632805347442627, dev loss: 2.862459659576416\n",
      "It count: 35000, train loss: 2.846811532974243, dev loss: 2.8458452224731445\n",
      "It count: 40000, train loss: 2.8395028114318848, dev loss: 2.837057590484619\n",
      "It count: 45000, train loss: 2.833026647567749, dev loss: 2.8312292098999023\n",
      "It count: 50000, train loss: 2.7294132709503174, dev loss: 2.72556734085083\n",
      "It count: 55000, train loss: 2.596628427505493, dev loss: 2.5883986949920654\n",
      "It count: 60000, train loss: 2.553548574447632, dev loss: 2.546391010284424\n",
      "It count: 65000, train loss: 2.5307154655456543, dev loss: 2.5215678215026855\n",
      "It count: 70000, train loss: 2.507373571395874, dev loss: 2.496030807495117\n",
      "It count: 75000, train loss: 2.501268148422241, dev loss: 2.4914135932922363\n",
      "It count: 80000, train loss: 2.499107837677002, dev loss: 2.489222288131714\n",
      "It count: 85000, train loss: 2.494985342025757, dev loss: 2.485642671585083\n",
      "It count: 90000, train loss: 2.4928090572357178, dev loss: 2.483083963394165\n",
      "It count: 95000, train loss: 2.4920568466186523, dev loss: 2.4827966690063477\n",
      "It count: 100000, train loss: 2.4893434047698975, dev loss: 2.4812769889831543\n",
      "It count: 105000, train loss: 2.4875905513763428, dev loss: 2.478553295135498\n",
      "It count: 110000, train loss: 2.487337589263916, dev loss: 2.4785051345825195\n",
      "It count: 115000, train loss: 2.487157106399536, dev loss: 2.478424549102783\n",
      "It count: 120000, train loss: 2.4870858192443848, dev loss: 2.4782705307006836\n",
      "It count: 125000, train loss: 2.4869630336761475, dev loss: 2.4782567024230957\n",
      "It count: 130000, train loss: 2.486833095550537, dev loss: 2.478120803833008\n",
      "It count: 135000, train loss: 2.4867308139801025, dev loss: 2.478065013885498\n",
      "It count: 140000, train loss: 2.486696243286133, dev loss: 2.4778146743774414\n",
      "It count: 145000, train loss: 2.4865963459014893, dev loss: 2.4778153896331787\n",
      "It count: 150000, train loss: 2.486546516418457, dev loss: 2.4777746200561523\n",
      "It count: 155000, train loss: 2.4864747524261475, dev loss: 2.4779770374298096\n",
      "It count: 160000, train loss: 2.4864397048950195, dev loss: 2.4776971340179443\n",
      "It count: 165000, train loss: 2.486524820327759, dev loss: 2.477560520172119\n",
      "It count: 170000, train loss: 2.486315965652466, dev loss: 2.4776360988616943\n",
      "It count: 175000, train loss: 2.4862754344940186, dev loss: 2.477443218231201\n",
      "It count: 180000, train loss: 2.4862356185913086, dev loss: 2.4775924682617188\n",
      "It count: 185000, train loss: 2.486140012741089, dev loss: 2.4775052070617676\n",
      "It count: 190000, train loss: 2.486116647720337, dev loss: 2.4773707389831543\n",
      "It count: 195000, train loss: 2.4861109256744385, dev loss: 2.47749924659729\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "loss_tr = []\n",
    "loss_dev = []\n",
    "stepi = []\n",
    "\n",
    "it_count=200000\n",
    "report_every_it = 1000\n",
    "print_every_it = 5000\n",
    "\n",
    "for i in range(it_count):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (mini_batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) + regularization * (W1**2).mean() + regularization * (W2**2).mean()\n",
    "    # print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1 if i < 100000 else 0.01 \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i%(report_every_it) == 0:\n",
    "        stepi.append(i)\n",
    "        loss_tr.append(total_loss(Xtr, Ytr))\n",
    "        loss_dev.append(total_loss(Xdev, Ydev))\n",
    "        if i%(print_every_it) == 0:\n",
    "            print(f\"It count: {i}, train loss: {loss_tr[-1]}, dev loss: {loss_dev[-1]}\")\n",
    "        \n",
    "        \n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "799e4ee6-709d-4d57-94b9-6f512d82b1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4764, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Train\n",
    "\n",
    "emb = C[Xtr] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64685ecd-b9ab-4be1-b560-09c7fb7cfc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4677, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Dev\n",
    "emb = C[Xdev] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bfa7568-73eb-4db6-80ad-1e14ce88ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Exercise 3 ------------------------------\n",
    "# Read the Bengio et al 2003 paper, implement and try any idea from the paper. Did it work?\n",
    "# Bengio et al 2003 paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "# Let's try this one:\n",
    "# 1. Decomposing the network in sub-networks, for example using a clustering of the words.\n",
    "# Training many smaller networks should be easier and faster.\n",
    "\n",
    "# Attempt:\n",
    "# Split the vocabulary into 2, in the middle --> chars1, chars2\n",
    "# Train two separate networks, depending on which vocabulary was the last letter in\n",
    "# Calculate loss\n",
    "# Sample from the network combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371b94fc-6b50-4110-aa08-c614981bb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcabdda0-03a2-45fe-84cb-a79a451c5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "262326ab-2c22-4404-b253-c43c61b42808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappins to/from integers\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "chars_split_i = int(len(chars)/2)\n",
    "a_chars = chars[:chars_split_i]\n",
    "b_chars = chars[chars_split_i:]\n",
    "\n",
    "a_chars_size = len(a_chars)\n",
    "b_chars_size = len(b_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "621d4039-3b4f-403a-a92a-b35785c5bac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words, a_chars, b_chars):\n",
    "    a_X, a_Y, b_X, b_Y = [], [], [], []\n",
    "    for w in words:\n",
    "\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            if ch in a_chars:\n",
    "                a_X.append(context)\n",
    "                a_Y.append(ix)\n",
    "            else:\n",
    "                b_X.append(context)\n",
    "                b_Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    a_X = torch.tensor(a_X)\n",
    "    a_Y = torch.tensor(a_Y)\n",
    "    b_X = torch.tensor(b_X)\n",
    "    b_Y = torch.tensor(b_Y)\n",
    "    return a_X, a_Y, b_X, b_Y\n",
    "    \n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "a_Xtr, a_Ytr, b_Xtr, b_Ytr = build_dataset(words[:n1], a_chars, b_chars)\n",
    "a_Xdev, a_Ydev, b_Xdev, b_Ydev = build_dataset(words[n1:n2], a_chars, b_chars)\n",
    "a_Xtest, a_Ytest, b_Xtest, b_Ytest = build_dataset(words[n2:], a_chars, b_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4565a47b-68d7-4707-92d9-5d3a537c3246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_size = 20\n",
    "hidden_layer_size = 50\n",
    "mini_batch_size = 1024\n",
    "regularization = 2\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, emb_size), generator=g)\n",
    "\n",
    "a_W1 = torch.randn((3*emb_size, hidden_layer_size), generator=g)\n",
    "a_b1 = torch.randn(hidden_layer_size, generator=g)\n",
    "a_W2 = torch.randn((hidden_layer_size,27), generator=g)\n",
    "a_b2 = torch.randn(27, generator=g)\n",
    "\n",
    "b_W1 = torch.randn((3*emb_size, hidden_layer_size), generator=g)\n",
    "b_b1 = torch.randn(hidden_layer_size, generator=g)\n",
    "b_W2 = torch.randn((hidden_layer_size,27), generator=g)\n",
    "b_b2 = torch.randn(27, generator=g)\n",
    "\n",
    "a_parameters = [C, a_W1, a_b1, a_W2, a_b2]\n",
    "b_parameters = [C, b_W1, b_b1, b_W2, b_b2]\n",
    "parameters = [C, a_W1, a_b1, a_W2, a_b2, b_W1, b_b1, b_W2, b_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "536ca974-5244-41b1-9649-3e7a3837ffda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9394"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41d4ef42-3220-42b8-b413-f047227122d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c92145aa-77cb-41d8-aa37-640218d190a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_loss(setX, setY, W1, b1, W2, b2):\n",
    "    emb = C[setX]\n",
    "    h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, setY) + regularization * (W1**2).mean() + regularization * (W2**2).mean()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18a4164d-eafa-4d6c-aaba-36f82878fed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It count: 0, train loss: 17.398569107055664, dev loss: 17.453584671020508\n",
      "It count: 5000, train loss: 2.2550759315490723, dev loss: 2.2553458213806152\n",
      "It count: 10000, train loss: 1.765372395515442, dev loss: 1.7680097818374634\n",
      "It count: 15000, train loss: 1.647465467453003, dev loss: 1.649801254272461\n",
      "It count: 20000, train loss: 1.612952709197998, dev loss: 1.6157277822494507\n",
      "It count: 25000, train loss: 1.609444260597229, dev loss: 1.6120115518569946\n",
      "It count: 30000, train loss: 1.607559084892273, dev loss: 1.610450029373169\n",
      "It count: 35000, train loss: 1.605723261833191, dev loss: 1.608553409576416\n",
      "It count: 0, train loss: 17.847610473632812, dev loss: 17.944202423095703\n",
      "It count: 5000, train loss: 2.405029058456421, dev loss: 2.430474281311035\n",
      "It count: 10000, train loss: 1.9169265031814575, dev loss: 1.94203519821167\n",
      "It count: 15000, train loss: 1.7964328527450562, dev loss: 1.820769190788269\n",
      "It count: 20000, train loss: 1.759708285331726, dev loss: 1.7838166952133179\n",
      "It count: 25000, train loss: 1.756131887435913, dev loss: 1.7802211046218872\n",
      "It count: 30000, train loss: 1.7540240287780762, dev loss: 1.779109001159668\n",
      "It count: 35000, train loss: 1.7521605491638184, dev loss: 1.7771316766738892\n",
      "It count: 0, train loss: 1.6619741916656494, dev loss: 1.6644401550292969\n",
      "It count: 5000, train loss: 1.589187502861023, dev loss: 1.5936001539230347\n",
      "It count: 10000, train loss: 1.581007957458496, dev loss: 1.5881175994873047\n",
      "It count: 15000, train loss: 1.5748043060302734, dev loss: 1.581451416015625\n",
      "It count: 20000, train loss: 1.5704493522644043, dev loss: 1.5794677734375\n",
      "It count: 25000, train loss: 1.5679532289505005, dev loss: 1.577021598815918\n",
      "It count: 30000, train loss: 1.5674508810043335, dev loss: 1.5765424966812134\n",
      "It count: 35000, train loss: 1.5669190883636475, dev loss: 1.5764553546905518\n",
      "It count: 0, train loss: 1.7535662651062012, dev loss: 1.7847356796264648\n",
      "It count: 5000, train loss: 1.729928970336914, dev loss: 1.7599557638168335\n",
      "It count: 10000, train loss: 1.719616413116455, dev loss: 1.7532318830490112\n",
      "It count: 15000, train loss: 1.7135473489761353, dev loss: 1.7487404346466064\n",
      "It count: 20000, train loss: 1.7087256908416748, dev loss: 1.7470176219940186\n",
      "It count: 25000, train loss: 1.705317497253418, dev loss: 1.7441593408584595\n",
      "It count: 30000, train loss: 1.7046979665756226, dev loss: 1.7443515062332153\n",
      "It count: 35000, train loss: 1.704230785369873, dev loss: 1.744591474533081\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "for _ in range(2):\n",
    "\n",
    "    for Xtr,Ytr,Xdev,Ydev,W1,b1,W2,b2, parameters in [(a_Xtr,a_Ytr,a_Xdev,a_Ydev,a_W1,a_b1,a_W2,a_b2, a_parameters),\n",
    "                                                     (b_Xtr,b_Ytr,b_Xdev,b_Ydev,b_W1,b_b1,b_W2,b_b2, b_parameters)]:\n",
    "\n",
    "        loss_tr = []\n",
    "        loss_dev = []\n",
    "        stepi = []\n",
    "\n",
    "        it_count=40000\n",
    "        report_every_it = 1000\n",
    "        print_every_it = 5000\n",
    "\n",
    "        for i in range(it_count):\n",
    "\n",
    "            # minibatch construct\n",
    "            ix = torch.randint(0, Xtr.shape[0], (mini_batch_size,))\n",
    "\n",
    "            # forward pass\n",
    "            emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "            h = torch.tanh(emb.view(-1, 3*emb_size) @ W1 + b1) # (32, 100)\n",
    "            logits = h @ W2 + b2\n",
    "            loss = F.cross_entropy(logits, Ytr[ix]) + regularization * (W1**2).mean() + regularization * (W2**2).mean()\n",
    "            # print(loss.item())\n",
    "\n",
    "            # backward pass\n",
    "            for p in parameters:\n",
    "                p.grad = None\n",
    "            loss.backward()\n",
    "\n",
    "            # update\n",
    "            # lr = lrs[i]\n",
    "            lr = 0.1 if i < it_count/2 else 0.01 \n",
    "            for p in parameters:\n",
    "                p.data += -lr * p.grad\n",
    "\n",
    "            # track stats\n",
    "            if i%(report_every_it) == 0:\n",
    "                stepi.append(i)\n",
    "                loss_tr.append(total_loss(Xtr, Ytr, W1, b1, W2, b2))\n",
    "                loss_dev.append(total_loss(Xdev, Ydev, W1, b1, W2, b2))\n",
    "                if i%(print_every_it) == 0:\n",
    "                    print(f\"It count: {i}, train loss: {loss_tr[-1]}, dev loss: {loss_dev[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d883f886-b1f8-4458-a143-3b3823b88e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "samples = []\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        logits = None\n",
    "        \n",
    "        if itos[context[2]] in a_chars:\n",
    "            h = torch.tanh(emb.view(1, -1) @ a_W1 + a_b1)\n",
    "            logits = h @ a_W2 + a_b2\n",
    "        else:\n",
    "            h = torch.tanh(emb.view(1, -1) @ b_W1 + b_b1)\n",
    "            logits = h @ b_W2 + b_b2\n",
    "            \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "        if ix==0:\n",
    "            break\n",
    "        out.append(ix)\n",
    "\n",
    "    samples.append(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17629c64-4366-4b5f-9586-04118844b297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample len: 100\n",
      "New Words len: 70\n",
      "New words: ['delie', 'adilah', 'ell', 'ich', 'caidella', 'kakaida', 'jacille', 'jaleigi', 'aai', 'iclaellah', 'jacia', 'daeka', 'kadel', 'akaigeli', 'fell', 'ihilie', 'balah', 'all', 'baleil', 'geal', 'feille', 'deli', 'fie', 'legaika', 'bhaab', 'aale', 'khallee', 'abellia', 'aah', 'keecle', 'jaed', 'lice', 'ellaileah', 'deagadihailee', 'cheille', 'lah', 'deah', 'aajhaad', 'gel', 'keilla', 'ficalcalli', 'kaddailiah', 'behlagha', 'kaelica', 'badelka', 'heacdaiaa', 'jagh', 'calle', 'ahlliklalia', 'kahayvonnonyxnossonnynorttonnytrrynntonnyszynnonyonnoztontustoroornorrynnsmontynontorynnovonnyronnynnnsurmynnnontonnysonsonnontryxsonntonnoynnottusswornozumnsonnystonnynnorustonnontonostytzrumourttynnormontrosstonvynnnonsynnnszorrynnsonnossonnonnyponnynsonortonnstyntonnorynnnwoustorynnonortonnonystoptrosssynnourmynontromononustontrystonnyxtonnonournntonnstonnysstonontryntonntzynnontonustonnzsonnystonyssonnyssontromyrosspsprnyntonnyvonnovonnyxsontryssonnontorynnoxtynnontonnyussonnyusporunustonnynnytonnypoonpynnyronstoourttymonnorvynnoynnnytorrynntzynnortomontrossonnstortonnyunonxynnnxoryntonnssonstonnonmyzonnyvontryvronzynnustynosmonnzurtonntzpontrossonnyszynnstornoryonnysonnynnsonnoxtonnyyzonnyntonnysusynnnnysontonnystynnoptonnyssonnoyrustynnorsonnowossynnzonnyynontrynnomontronosynnnsonnysttynnstynnonmorynnyxsonnmynnnyonnynsorrynsomustortuvonnomurttontryonnynzorystopzynnstonoryusturottymonyzynnynontynorrynnsynnsynnsustonysonnuynonsonnorynontzonnyonnzonnysonnysssporrynnonnysonsowossontrysonnoruwnnystoortontrystovynntynnnorronnoxtontoonstonnooryontrurtynnnzorynnonyyronnysunponvontorroztponnyzsia', 'elghaleigal', 'khala', 'bleigha', 'aaha', 'kalba', 'aice', 'kaelle', 'ick', 'beabella', 'liid', 'hea', 'deli', 'faahia', 'dalah', 'elee', 'dalee', 'aleilia', 'dae', 'ceel', 'gadah']\n"
     ]
    }
   ],
   "source": [
    "new_words = [sample for sample in samples if sample not in words]\n",
    "\n",
    "print(f\"Sample len: {len(samples)}\")\n",
    "print(f\"New Words len: {len(new_words)}\")\n",
    "\n",
    "print(\"New words:\", new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cb8529c9-0886-4aee-98ef-d7216621df78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interesting... it somewhat feels like it worked, but I don't understand how I get so low losses\n",
    "\n",
    "# Seems like training a_char, then b_char then a_char again helps lower the loss faster\n",
    "# The training takes 2:45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d08cef-52e9-41f0-94e5-10e1a8e52537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
